---
title: "Regression Models Course Project - Kevin O'Leary"
output:
  pdf_document:
    fig_crop: no
    fig_height: 3.55
    fig_width: 6
geometry: tmargin=1cm, lmargin=1cm, rmargin=1cm, bmargin = 1.5cm
fontsize: 5pt
---

**Executive Summary

"Is an automatic or manual transmission better for MPG"
"Quantify the MPG difference between automatic and manual transmissions"

##------------------------------------------------------------------------------

# Create factors for the categorial variables
cars <- mtcars
cars$am <- factor(mtcars$am, levels=c(0,1), labels=c("automatic", "manual"))
cars$cyl <- factor(mtcars$cyl)
cars$vs <- factor(mtcars$vs, levels=c(0,1), labels=c("s", "v"))
cars$gear <- factor(mtcars$gear)
cars$carb <- factor(mtcars$carb)


summary(lm(mpg ~ . , data = cars))$coefficients

summary(lm(mpg ~ am, data = cars))

Here automatic is the reference category since it's the first level by alphabetical order.

cars$am2 <- relevel(cars$am, "manual")

summary(lm(mpg ~ am2, data = cars))

require(datasets); data(cars); require(GGally); require(ggplot2)
g = ggpairs(cars, lower = list(continuous = "smooth"),params = c(method = "loess"))
g

Our models estimates an expected 2.5 increase in mpg for every 1% increase in percentage of males involved in agriculture in holding the remaining variables constant.

require(datasets); data(swiss); require(GGally); require(ggplot2)
g = ggpairs(swiss, lower = list(continuous = "smooth"),params = c(method = "loess"))
g

summary(lm(Fertility ~ . , data = swiss))

summary(lm(Fertility ~ . , data = swiss))$coefficients

Our models estimates an expected 0.17 decrease in standardized fertility for every 1% increase in percentage of males involved in agriculture in holding the remaining variables constant.
##------------------------------------------------------------------------------

To get a quick glimpse of the answer, we compute the mean of mpg by transmission
and we indeed find that manual (1) has the highest corresponding mpg on average.

aggregate(mpg~am, data=cars, mean)

A simple plot also highlights the difference.

plot(cars$mpg~cars$am, xlab="Transmission", ylab="MPG", main="Boxplots of MPG by Transmission")

According to the EPA's 'Factors Affecting Automotive Fuel Economy' document, the 
most important vehicle design features affecting fuel economy are vehicle weight 
and engine displacement. We'll need to examine the data again to factor out 
these features and view the effect of gearing on it's own.

cars <- mtcars
cars$am <- factor(mtcars$am, levels=c(0,1), labels=c("auto", "manual"))
cars$cyl <- factor(mtcars$cyl)
cars$vs <- factor(mtcars$vs, levels=c(0,1), labels=c("s", "v"))
cars$gear <- factor(mtcars$gear)
cars$carb <- factor(mtcars$carb)

Let's have a look at our data and tease out the significant factors affecting mpg.

names(cars)

Let's first create a maximal model that includes all the predictors.

maxmod = lm(mpg ~ am + cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb, data=cars)
summary(maxmod)

We can then use the step function to select the best models by relative likelihood.

minmod <- step(maxmod, direction="backward")

summary(minmod)

Now all the predictors are significant and we have reached the minimal adequate model.
We have an adjusted R squared of 0.84 which suggests that our model is quite accurate.

resmod = resid(minmod)

plot(cars$mpg, resmod, ylab="Residuals", xlab="MPG", main="Residuals V Fitted") 
abline(0, 0) 

These residuals appear exhibit homogeneity, normality, and independence. 

plot(fitted(minmod), residuals(minmod), xlab = "Fitted Values", ylab = "Residuals", main="Residuals V Fitted")
abline(h=0, lty=2)
lines(smooth.spline(fitted(minmod), residuals(minmod)))


Now, using ANOVA, we will compare the base model with only am as the predictor variable and the best model obtained above:

basemod <- lm(mpg ~ am, data = cars)

anova(basemod, minmod)

confint(minmod)

Now that we know our major factors, let's create a continuous and discrete pairwise plot to see what's going on.

library(GGally)
subcars = subset(cars, select=c("mpg", "am", "cyl", "hp", "wt"))
ggpairs(subcars, diag=list(continuous="density", discrete="bar"), axisLabels="show")

Negative, almost linear correlation between both weight, horsepower and mpg, the higher the weight and horsepower, the lower the mpg.
Higher average fuel economy for manual cars and lower mpg as the number of cylinders rises.
Automatic cars are generally heavier than manual and the number of cylinders also results in greater weight.
Weight is positively correlated with horsepower.
Horsepower appears to be greater for automatic cars, however, manual cars have high outliers that shouldn't be ignored. This is due to the fact that in our data, automatic cars tend to have more cylinders. Multi-colinearity

A key observation is that automatic cars in our data tend to have more cylinders and manual tend to have fewer and an increase in cylinders is correlated with an increase in weight which in turn is negatively correlated with mpg. Whatever tendency automatic cars have for being heavier than manuals is compounded by this issue in the data.

summary(lm(data=mtcars, formula=mpg ~ am))


summary(minmod)

summary(lm(mpg ~ am -1, data = cars))$coef ##returns the mean mpg for each factor, without the intercept

summary(lm(mpg ~ am, data = cars))$coef

Here automatic is the reference category since it's the first level by alphabetical order.

The coefficient shows the difference in mean mpg between automatic, the reference category, and manual. The intercept is the mean of automatic.

cars$am2 <- relevel(cars$am, "manual")

summary(lm(mpg ~ am2, data = cars))$coef

17.147+7.244

##------------------------------------------------------------------------------

It might seem that but there are several other factors that we need to consider.



##------------------------------------------------------------------------------

Let's first prepare the data for use.

mtcars$am <- factor(mtcars$am)
mtcars$gear <- factor(mtcars$gear)
levels(mtcars$am) <- c("auto", "manual")
mtcars



##------------------------------------------------------------------------------

x1 = auto$mpg
y1 = auto$am

x2 = manual$mpg
y2 = manual$am


linearauto = lm(y1 ~ x1) ##outcome/predictor 
linearman = lm(y2 ~ x2) ##outcome/predictor 

modelcomp = anova(linearauto, linearman)

##------------------------------------------------------------------------------

library(ggplot2)

g = ggplot(mtcars, aes(x = am, y = mpg ),)

g = g + xlab("Transmission")
g = g + ylab("MPG")
g = g + geom_point(size = 6, colour = "black", alpha = 0.2 )
g = g + geom_point(size = 5, colour = "blue", alpha = 0.2 )
g = g + geom_smooth(method = "lm", colour = "black")
g






##Get a 95% confidence interval for the expected mpg at the 
##average weight.

x = mtcars$am
y = mtcars$mpg

linearmod = lm(y ~ x) ##outcome/predictor

summary(linearmod)

newdata = data.frame(x=mean(x))

predict(linearmod, newdata, interval = ("confidence"))









First we need to find the mean of 40 exponential distributions and iterate 1000 
times. Then we can simply take the mean of this vector and compare against the
theoretical mean of 1/lambda. Here we find close agreement of **4.986508** and 
**5**, respectively. 
```{r}
set.seed(42) ##specify random seed state 
lambda=.2 ##set lambda value
expno=40 ##set number of exponentials
simno=1000 ##set number of simulations

exdis = rep(NA,simno) ##create an empty vector with 1000 elements

for (i in 1:simno){ ##for every element in exdis...
  exdis[i] = mean(rexp(expno,lambda)) ##populate with the mean of 40 exponentials
}

samean=mean(exdis) ##assign the mean of exdis to samean
theomean=1/lambda ##assign the theoretical mean to theomean
```
To appreciate this graphically, we first need to calculate, then plot, the 
moving average, or cumulative mean of our exponentials. The CLT suggests that 
this cumulative mean should converge on the theoretical mean as the iterations 
increase, which is in agreement with the graphic below.
```{r}
cumean = cumsum(exdis)/seq_along(exdis) ##mean of each cumulative sum

par(mar=c(3,11,1,1)+1.5, cex=0.7) ##set margins and font size

plot(seq_along(exdis), cumean, type="l",
     main="Means of 40 Exponentials",xlab="No of Iterations", 
     ylab="Mean")
       abline(h=theomean, col="goldenrod3") ##theoretical mean line
     legend("topright", legend=c("Theoretical Mean","Sample Mean"),
       col=c("goldenrod3","black"), lwd=c(2,2), cex=.7, bty="n")
```

**2. Show how variable the sample is (via variance) and compare it to the theoretical variance of the distribution.**

Similar to above, we can take the variance of our vector and compare against the 
theoretical variance of 40 distributions. In this case we find that **0.6344405** 
and **0.625** match closely.

Next we need to calculate the cumulative variance to plot the change in 
variance with iteration. As was seen above, the greater the number of 
iterations, the closer the sample variance gets to the theoretical variance.
```{r}
samvar = var(exdis) ##assign sample variance to samvar
theovar = 1/((lambda^2)*expno) ##theoretical variance for 40 distributions
cumvar = cumsum((exdis-samean)^2)/(seq_along(exdis)-1) ##cumulative variance

par(mar=c(3,11,1,1)+1.5, cex=0.7) ##set margins and font size

plot(seq_along(exdis),cumvar,type="l",
     main="Variance of 40 Exponentials",
     xlab="No of Iterations", ylab="Variance")
abline(h=theovar, col="goldenrod3") ##theoretical variance line
legend("topright", legend=c("Theoretical Variance","Sample Variance"),
       col=c("goldenrod3","black"), lwd=c(2,2),cex=.7,bty="n")
```
\newpage

**3. Show that the distribution is approximately normal.**

Below we have superimposed a normal distribution curve with the theoretical mean
and standard deviation over a histogram of our data. We can see that both are in 
general agreement with the data, meaning that the data is approximately normal.
```{r}
par(mar=c(3,11,1,1)+1.5, cex=0.7) ##set margins and font size

hist(exdis,breaks=30, freq=FALSE, col="slategray2", 
     main="Comparison of Sample Exponentials to Normal Distribution", xlab="Value",ylab="Density")
curve(dnorm(x, mean=theomean, sd=sqrt(theovar)),
      add=TRUE,lwd=2, col="goldenrod3") ##normal distribution curve
abline(v=theomean,lwd=2,col="violetred3") ##theoretical mean line
legend("topright", legend=c("Theoretical Mean","Theoretical Distribution"),
       col=c("violetred3","goldenrod3"), lty=c(1,1),lwd=c(2,2),cex=.8,bty="n")
```
